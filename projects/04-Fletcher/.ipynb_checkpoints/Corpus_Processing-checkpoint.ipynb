{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def remove_newline_characters(string):\n",
    "    lines = str(string).split('\\n')\n",
    "    paragraph = ''\n",
    "    for i in lines:\n",
    "        paragraph += (str(i)+' ')\n",
    "    return paragraph\n",
    "\n",
    "df = pd.read_csv('aaai_papers.csv')\n",
    "#df2 = pd.read_csv('aaai_papers_2013.csv')\n",
    "\n",
    "#df1 = pd.DataFrame({'title':df2['Title'],'authors':np.empty(len(df2['Title'])),'groups':np.empty(len(df2['Title'])),\n",
    " #                  'topics':np.empty(len(df2['Title'])),'abstract':df2['Abstract'],'keywords':np.empty(len(df2['Title']))})\n",
    "\n",
    "#df1 = df1[df.columns]\n",
    "\n",
    "\n",
    "for i in df.columns[2:]:        \n",
    "    df[i]=df[i].map(remove_newline_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novel Machine Learning Algorithms (NMLA) 18\n",
      "AI and the Web (AIW) 6\n",
      "Game Theory and Economic Paradigms (GTEP) 8\n",
      "NLP and Text Mining (NLPTM) 1\n",
      "Knowledge Representation and Reasoning (KRR) 7\n",
      "Machine Learning Applications (MLA) 8\n",
      "Vision (VIS) 4\n",
      "NLP and Machine Learning (NLPML) 2\n",
      "Heuristic Search and Optimization (HSO) 2\n",
      "Search and Constraint Satisfaction (SCS) 5\n",
      "Reasoning under Uncertainty (RU) 6\n",
      "Planning and Scheduling (PS) 5\n",
      "Applications (APP) 4\n",
      "Multiagent Systems (MAS) 6\n",
      "Computational Sustainability and AI (CSAI) 2\n",
      "Cognitive Modeling (CM) 0\n",
      "Cognitive Systems (CS) 1\n",
      "Humans and AI (HAI) 1\n",
      "Robotics (ROB) 2\n",
      "Game Playing and Interactive Entertainment (GPIE) 0\n",
      "NLP and Knowledge Representation (NLPKR) 1\n",
      "Human-Computation and Crowd Sourcing (HCC) 0\n"
     ]
    }
   ],
   "source": [
    "#making a list of unique groups (topics)\n",
    "import csv\n",
    "\n",
    "topic_list = []\n",
    "paper_topics = []\n",
    "with open('aaai_papers.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        topics = str(row['groups']).split('\\n')\n",
    "        paper_topics.append(topics[-1])\n",
    "        for topic in topics:\n",
    "            paper_topics.append(topic)\n",
    "            if topic not in topic_list:\n",
    "                topic_list.append(topic)\n",
    "\n",
    "topic_list.remove('')\n",
    "for i in topic_list:\n",
    "    print i, int(paper_topics.count(i)/float(len(paper_topics))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novel Machine Learning Algorithms \n",
      "AI and the Web \n",
      "Game Theory and Economic Paradigms \n",
      "NLP and Text Mining \n",
      "Knowledge Representation and Reasoning \n",
      "Machine Learning Applications \n",
      "Vision \n",
      "NLP and Machine Learning \n",
      "Heuristic Search and Optimization \n",
      "Search and Constraint Satisfaction \n",
      "Reasoning under Uncertainty \n",
      "Planning and Scheduling \n",
      "Applications \n",
      "Multiagent Systems \n",
      "Computational Sustainability and AI \n",
      "Cognitive Modeling \n",
      "Cognitive Systems \n",
      "Humans and AI \n",
      "Robotics \n",
      "Game Playing and Interactive Entertainment \n",
      "NLP and Knowledge Representation \n",
      "Human-Computation and Crowd Sourcing \n"
     ]
    }
   ],
   "source": [
    "for i in topic_list:\n",
    "    print i.split('(')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"aaai_topics.pkl\", 'w') as datafile:\n",
    "    pickle.dump(topic_list, datafile)\n",
    "    \n",
    "with open(\"topic_of_abstract.pkl\", 'w') as datafile:\n",
    "    pickle.dump(paper_topics, datafile)\n",
    "\n",
    "abstracts = np.array(df['abstract'])\n",
    "with open(\"aaai_abstracts.pkl\", 'w') as datafile:\n",
    "    pickle.dump(abstracts,datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_newline_characters(string):\n",
    "    lines = str(string).split('\\n')\n",
    "    paragraph = ''\n",
    "    for i in lines:\n",
    "        paragraph += (str(i)+' ')\n",
    "    return paragraph\n",
    "\n",
    "# import pymongo\n",
    "\n",
    "# client = pymongo.MongoClient('ec2-54-84-1-137.compute-1.amazonaws.com', 27017)\n",
    "# db = client.aaai\n",
    "# papers = db.papers2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#dict_list = [] #list of papers imported as dictionary\n",
    "# with open('aaai_papers.csv') as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     for row in reader:\n",
    "#         paper_dict = {}\n",
    "#         for k,v in row.iteritems():\n",
    "#             paper_dict[k] =remove_newline_characters(v)\n",
    "#         papers.insert(paper_dict)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"clusters.pkl\", 'r') as datafile:\n",
    "    clusters = pickle.load(datafile)\n",
    "\n",
    "with open(\"rec.pkl\", 'r') as datafile:\n",
    "    recs = pickle.load(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "print len(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 398 398\n"
     ]
    }
   ],
   "source": [
    "title_array = np.array(df['title'])\n",
    "author_array = np.array(df['authors'])\n",
    "cluster = np.array(clusters)\n",
    "rec1 = []\n",
    "rec2 = []\n",
    "rec3 =[]\n",
    "abstract_array = np.array(df['abstract'])\n",
    "\n",
    "for i,j in enumerate(recs):\n",
    "    rec1.append(str(title_array[j[0][0]])+' by ' + str(author_array[j[0][0]]))\n",
    "    rec2.append(str(title_array[j[1][0]])+' by ' + str(author_array[j[1][0]]))\n",
    "    rec3.append(str(title_array[j[2][0]])+' by ' + str(author_array[j[2][0]]))\n",
    "    \n",
    "print len(rec1),len(rec2),len(rec3)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st_rec</th>\n",
       "      <th>2nd_rec</th>\n",
       "      <th>3rd_rec</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>cluster</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Source Free\" Transfer Learning for Text Class...</td>\n",
       "      <td>Latent Low-Rank Bi-Directional Transfer Subspa...</td>\n",
       "      <td>Supervised Transfer Sparse Coding by Maruan Al...</td>\n",
       "      <td>Transfer learning considers related but distin...</td>\n",
       "      <td>Mehmet Gönen and Adam A. Margolin</td>\n",
       "      <td>10</td>\n",
       "      <td>Kernelized Bayesian Transfer Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Latent Low-Rank Bi-Directional Transfer Subspa...</td>\n",
       "      <td>Kernelized Bayesian Transfer Learning by Mehme...</td>\n",
       "      <td>Supervised Transfer Sparse Coding by Maruan Al...</td>\n",
       "      <td>Transfer learning uses relevant auxiliary data...</td>\n",
       "      <td>Zhongqi Lu, Yin Zhu, Sinno Pan, Evan Xiang, Yu...</td>\n",
       "      <td>10</td>\n",
       "      <td>\"Source Free\" Transfer Learning for Text Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On the Incompatibility of Efficiency and Strat...</td>\n",
       "      <td>Simultaneous Cake Cutting by Eric Balkanski, S...</td>\n",
       "      <td>On the Axiomatic Characterization of Runoff Vo...</td>\n",
       "      <td>The probabilistic serial (PS) rule is one of t...</td>\n",
       "      <td>Haris Aziz and Paul Stursberg</td>\n",
       "      <td>14</td>\n",
       "      <td>A Generalization of Probabilistic Serial to Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online Social Spammer Detection by Xia Hu, Jil...</td>\n",
       "      <td>Incentivizing High-quality Content from Hetero...</td>\n",
       "      <td>Detecting information-dense texts in multiple ...</td>\n",
       "      <td>As the rapid growth of online social media att...</td>\n",
       "      <td>Liao Lizi, Jing Jiang, Ying Ding, Heyan Huang ...</td>\n",
       "      <td>16</td>\n",
       "      <td>Lifetime Lexical Variation in Social Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low-Rank Tensor Completion with Discriminant A...</td>\n",
       "      <td>Generalized Higher-Order Tensor Decomposition ...</td>\n",
       "      <td>Video Recovery via Low-Rank Tensor Completion ...</td>\n",
       "      <td>In this paper, we study the low-rank tensor co...</td>\n",
       "      <td>Xiaoqin Zhang, Zhengyuan Zhou, Di Wang and Yi Ma</td>\n",
       "      <td>10</td>\n",
       "      <td>Hybrid Singular Value Thresholding for Tensor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             1st_rec  \\\n",
       "0  \"Source Free\" Transfer Learning for Text Class...   \n",
       "1  Latent Low-Rank Bi-Directional Transfer Subspa...   \n",
       "2  On the Incompatibility of Efficiency and Strat...   \n",
       "3  Online Social Spammer Detection by Xia Hu, Jil...   \n",
       "4  Low-Rank Tensor Completion with Discriminant A...   \n",
       "\n",
       "                                             2nd_rec  \\\n",
       "0  Latent Low-Rank Bi-Directional Transfer Subspa...   \n",
       "1  Kernelized Bayesian Transfer Learning by Mehme...   \n",
       "2  Simultaneous Cake Cutting by Eric Balkanski, S...   \n",
       "3  Incentivizing High-quality Content from Hetero...   \n",
       "4  Generalized Higher-Order Tensor Decomposition ...   \n",
       "\n",
       "                                             3rd_rec  \\\n",
       "0  Supervised Transfer Sparse Coding by Maruan Al...   \n",
       "1  Supervised Transfer Sparse Coding by Maruan Al...   \n",
       "2  On the Axiomatic Characterization of Runoff Vo...   \n",
       "3  Detecting information-dense texts in multiple ...   \n",
       "4  Video Recovery via Low-Rank Tensor Completion ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Transfer learning considers related but distin...   \n",
       "1  Transfer learning uses relevant auxiliary data...   \n",
       "2  The probabilistic serial (PS) rule is one of t...   \n",
       "3  As the rapid growth of online social media att...   \n",
       "4  In this paper, we study the low-rank tensor co...   \n",
       "\n",
       "                                              author  cluster  \\\n",
       "0                  Mehmet Gönen and Adam A. Margolin       10   \n",
       "1  Zhongqi Lu, Yin Zhu, Sinno Pan, Evan Xiang, Yu...       10   \n",
       "2                      Haris Aziz and Paul Stursberg       14   \n",
       "3  Liao Lizi, Jing Jiang, Ying Ding, Heyan Huang ...       16   \n",
       "4   Xiaoqin Zhang, Zhengyuan Zhou, Di Wang and Yi Ma       10   \n",
       "\n",
       "                                               title  \n",
       "0              Kernelized Bayesian Transfer Learning  \n",
       "1  \"Source Free\" Transfer Learning for Text Class...  \n",
       "2  A Generalization of Probabilistic Serial to Ra...  \n",
       "3         Lifetime Lexical Variation in Social Media  \n",
       "4  Hybrid Singular Value Thresholding for Tensor ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstr_info = pd.DataFrame({'title':title_array,'author':author_array,'cluster':cluster,'1st_rec':rec1,\n",
    "                          '2nd_rec':rec2,'3rd_rec':rec3,'abstract':abstract_array})\n",
    "abstr_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "clustered_abstr = np.array(abstr_info[abstr_info.cluster==10]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "import re\n",
    "\n",
    "def word_count(string):\n",
    "    d = defaultdict(int)\n",
    "    count_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1,1), stop_words='english',\n",
    "                                  token_pattern='\\\\b[a-z][a-z]+\\\\b')\n",
    "    s = count_vectorizer.get_stop_words()\n",
    "    words = string.split()\n",
    "    for i in words:\n",
    "        if (i not in s) and (re.match('\\\\b[a-z][a-z]+\\\\b',i)):\n",
    "            i.replace('\\\"','')\n",
    "            d[i.lower()]+=1\n",
    "    return sorted(d.items(), key=operator.itemgetter(1),reverse = True)\n",
    "    \n",
    "def cluster_word_count(cluster_num,abstr_info):\n",
    "    clustered_abstr = np.array(abstr_info[abstr_info.cluster==cluster_num]['abstract'])\n",
    "    concat_abstr=''\n",
    "    for i in clustered_abstr:\n",
    "        concat_abstr+=i\n",
    "    return word_count(concat_abstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in range(22):\n",
    "    words , counts = zip(*cluster_word_count(cluster,abstr_info))\n",
    "    total_words = ''\n",
    "\n",
    "    for i,j in enumerate(words):\n",
    "        total_words += (j + ' ')*counts[i]\n",
    "    \n",
    "    f = \"words\"+str(cluster)+\".csv\"\n",
    "    resultFile = open(f,'wb')\n",
    "    wr = csv.writer(resultFile)\n",
    "    wr.writerow(total_words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame({'word':np.array(total_words.split())})\n",
    "words_df.to_csv('words3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
