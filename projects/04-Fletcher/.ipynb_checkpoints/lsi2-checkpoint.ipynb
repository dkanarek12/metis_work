{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector Space Models for Text - LSI and Word2vec\n",
    "\n",
    "Yesterday, we used [**Latent Dirichlet Allocation (LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) in [gensim](http://radimrehurek.com/gensim/index.html) to map text documents ([20 Newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)) from a **word space** to a **topic space** that could give us the **topic distribution** of different documents in our corpus as well as allow us to make **conceptual comparisons** of documents in the reduced topic space.\n",
    "\n",
    "Today, we'll continue mapping text documents from a highly dimensional word (or token) space into a much reduced **semantic space** which allows us to make valuable **conceptual comparisons** between arbitrary blocks of text in this new vector space.\n",
    "\n",
    "Thus the **input is a large corpus of text documents** and the **output is a reduced semantic space for those input documents and words**.  These starting/ending points are constant, but we'll take 2 different approaches for the process in between:\n",
    "1.  [**Latent Semantic Indexing (LSI)**](https://en.wikipedia.org/wiki/Latent_semantic_analysis) - performs a [**Singular Value Decomposition (SVD)**](https://en.wikipedia.org/wiki/Singular_value_decomposition) on a [**document-term matrix**](https://en.wikipedia.org/wiki/Document-term_matrix) with [**TFIDF Weightings**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to map all the terms in the corpus into a reduced **term space** and all the documents into a reduced **document space**.  \n",
    "    - The 2 spaces are related by a simple transformation, so we can perform arbitrary **term-term**, **doc-doc**, and **doc-term comparisons** via [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "    - These 2 spaces make up the **\"dual space\"**\n",
    "        - Every document is the weighted sum of all of its terms\n",
    "        - Every term is the weighted sum of all the documents it occurs in (very useful!)\n",
    "2.  [**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec) - uses a neural network to yield **term space**\n",
    "    - Has additional nice properties of term vectors, such as conceptual additivity (see below)\n",
    "\n",
    "## Goals\n",
    "- Continue to use gensim to implement text modeling\n",
    "- Build an LSI vector space from a training set\n",
    "- Use the LSI space to compare terms and documents to one another conceptually\n",
    "- Use the LSI space to perform document clustering and classification\n",
    "- Use Word2vec to create a vector space for words in a training set\n",
    "- Use the Word2vec space to do simple comparisons between different combinations of words\n",
    "- Discuss various other considerations, tasks, and extensions for VSMs like LSI and Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI\n",
    "For us, LSI will consist of the following steps:\n",
    "1. Getting the data\n",
    "2. Text Preprocessing\n",
    "3. Create Document-Term matrix\n",
    "4. Apply TFIDF weights to document-term matrix\n",
    "5. Perform SVD on TFIDF matrix\n",
    "6. Use resulting term and document space\n",
    "    - Term-term comparisons\n",
    "    - Term-document comparisons\n",
    "    - Document-document comparisons\n",
    "    - Document clustering\n",
    "    - Document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data\n",
    "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"aaai_topics.pkl\", 'r') as datafile:\n",
    "    categories = pickle.load(datafile)\n",
    "\n",
    "with open(\"aaai_abstracts.pkl\", 'r') as datafile:\n",
    "    abstracts = pickle.load(datafile)\n",
    "\n",
    "for i, j in enumerate(abstracts):\n",
    "    abstracts[i]=str(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We'll need to generate a term-document matrix of word (token) counts for use in LSI.  LSI requires that we go a step further in our processing by adding TFIDF weightings to our counts matrix.  We could just use the `TfidfVectorizer` in `sklearn`, but `gensim` has its own `TfidfModel`, so we'll save that for later.\n",
    "\n",
    "We'll use `sklearn`'s `CountVectorizer` to generate our term-document matrix of counts. We'll make use of a few parameters to accomplish the following preprocessing of the text documents all within the `CountVectorizer`:\n",
    "* `analyzer=word`: Tokenize by word\n",
    "* `ngram_range=(1,2)`: Keep all 1 and 2-word grams\n",
    "* `stop_words=english`: Remove all English stop words\n",
    "* `token_pattern=\\\\b[a-z][a-z]+\\\\b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 2), stop_words='english',\n",
    "                                  token_pattern='\\\\b[a-z][a-z]+\\\\b')\n",
    "count_vectorizer.fit(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34527, 398)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "ng_vecs = count_vectorizer.transform(abstracts).transpose()\n",
    "ng_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to gensim\n",
    "We need to convert our sparse `scipy` matrix to a `gensim`-friendly object called a Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(ng_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map matrix rows to words (tokens)\n",
    "We need to save a mapping (dict) of row id to word (token) for later use by gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "LSI requires us to go one step further than LDA in preprocessing, we need to calculate [TFIDF weights](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) from our word counts term-document matrix.  Here's how we do it in gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a TFIDF transformer from our word counts (equivalent to \"fit\" in sklearn)\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to give each document vectors in the \"TFIDF space\" we need to actually do the transform step with our TfidfModel like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a TFIDF vector for all documents from the original corpus (\"transform\" in sklearn)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TFIDF\n",
    "At this point, we already have mapped our original inputs (text documents) into a vector space (TFIDF space) with a dimensionality equal to the total number of unique terms in our corpus.  That means that, in theory, we could go ahead and see if this vector space can tell us anything interesting.  We could try **comparing documents** by something like [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).  We could try machine learning methods like document **clustering** on the vectors, or **classification/regression** if we have labeled documents.\n",
    "\n",
    "A common approach for document classification is to now try [Naive Bayes Classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification) using our TFIDF space.  This can work quite well if documents of a given class have very class-specific words such that only documents of a given class use them frequently.  For instance, this often quite well in detecting authors by word choice.\n",
    "\n",
    "##### Curse of Dimensionality\n",
    "Although TFIDF (and even counts) gives us vectors, as you might guess we're likely to run into the curse of dimensionality.  In a dataset of any reasonable size, there are likely to be tons of unique terms and thus really high-dimensional vectors.  Naive Bayes is highly resistant to the curse, thus explaining why it can work well for the right dataset.  \n",
    "\n",
    "However, we should be heavily inclined now to try some dimensionality reduction to alleviate the curse...  \n",
    "Enter: SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "For LSI, the SVD does our dimensionality reduction on the TFIDF space.  In effect, it reduces the space by coupling together terms of very similar meaning and thus alleviating redundant and collinear features (terms).\n",
    "\n",
    "So now that we've taken care of the TFIDF bit, let's crank through the SVD and build the LSI space in gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build an LSI space from the input TFIDF matrix, mapping of row id to word, and num_topics\n",
    "# num_topics is the number of dimensions to reduce to after the SVD\n",
    "# Analagous to \"fit\" in sklearn, it primes an LSI space\n",
    "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained LSI space, we want to do the transform step to figure out where all of the original documents lie in that num_topics=300 dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve vectors for the original tfidf corpus in the LSI space (\"transform\" in sklearn)\n",
    "lsi_corpus = lsi[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump the resulting document vectors into a list so we can take a look\n",
    "doc_vecs = [doc for doc in lsi_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Similarity Between Documents\n",
    "Now that we have vectors in the LSI space, we can compare any indexed document to any other index document in the space via cosine similarity.  gensim allows us to do this like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "# Create an index transformer that calculates similarity based on our space\n",
    "index = similarities.MatrixSimilarity(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the sorted list of cosine similarities to the first document\n",
    "recs = []\n",
    "for i in range(398):\n",
    "    sims = sorted(enumerate(index[doc_vecs[i]]), key=lambda item: -item[1])\n",
    "    recs.append(sims[1:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"rec.pkl\", 'w') as datafile:\n",
    "    pickle.dump(recs, datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How'd we do??  Let's check the most similar doc!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transfer learning considers related but distinct tasks defined on heterogenous domains and tries to transfer knowledge between these tasks to improve generalization performance. It is particularly useful when we do not have sufficient amount of labeled training data in some tasks, which may be very costly, laborious, or even infeasible to obtain. Instead, learning the tasks jointly enables us to effectively increase the amount of labeled training data. In this paper, we formulate a kernelized Bayesian transfer learning framework that is a principled combination of kernel-based dimensionality reduction models with task-specific projection matrices to find a shared subspace and a coupled classification model for all of the tasks in this subspace. Our two main contributions are: (i) two novel probabilistic models for binary and multiclass classification, and (ii) very efficient variational approximation procedures for these models. We illustrate the generalization performance of our algorithms on two different applications. In computer vision experiments, our method outperforms the state-of-the-art algorithms on nine out of 12 benchmark supervised domain adaptation experiments defined on two object recognition data sets. In cancer biology experiments, we use our algorithm to predict mutation status of important cancer genes from gene expression profiles using two distinct cancer populations, namely, patient-derived primary tumor data and in-vitro-derived cancer cell line data. We show that we can increase our generalization performance on primary tumors using cell lines as an auxiliary data source. '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transfer learning uses relevant auxiliary data to help the learning task in a target domain where labeled data are usually insufficient to train an accurate model. Given appropriate auxiliary data, researchers have proposed many transfer learning models. How to find such auxiliary data, however, is of little research in the past. In this paper, we focus on this auxiliary data retrieval problem, and propose a transfer learning framework that effectively selects helpful auxiliary data from an open knowledge space (e.g. the World Wide Web). Because there is no need of manually selecting auxiliary data for different target domain tasks, we call our framework Source Free Transfer Learning (SFTL). For each target domain task, SFTL framework iteratively queries for the helpful auxiliary data based on the learned model and then updates the model using the retrieved auxiliary data. We highlight the automatic constructions of queries and the robustness of the SFTL framework. Our experiments on the 20 NewsGroup dataset and the Google search snippets dataset suggest that the new framework is capable to have the comparable performance to those state-of-the-art methods with dedicated selections of auxiliary data. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well my word, that looks pretty darn similar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with LSI Vectors\n",
    "We have (very good, 300-dimensional) vectors for our documents now!  So we can do any machine learning we want on our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the gensim-style corpus vecs to a numpy array for sklearn manipulations\n",
    "X = matutils.corpus2dense(lsi_corpus, num_terms=200).transpose()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering LSI Vectors:\n",
    "Let's try clustering our documents with `sklearn` to see if we can notice any obvious clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1\n",
      "21 1\n",
      "1 6\n",
      "0 2\n",
      "3 5\n",
      "2 4\n",
      "5 1\n",
      "4 4\n",
      "7 4\n",
      "6 6\n",
      "9 8\n",
      "8 2\n",
      "11 3\n",
      "10 22\n",
      "13 1\n",
      "12 2\n",
      "15 4\n",
      "14 5\n",
      "17 0\n",
      "16 9\n",
      "19 1\n",
      "18 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster_quantity = defaultdict(int)\n",
    "agg_clust = AgglomerativeClustering(n_clusters = 22,linkage = 'complete',affinity='l1')\n",
    "clusters = agg_clust.fit_predict(X)\n",
    "\n",
    "for i in clusters:\n",
    "    cluster_quantity[str(i)]+=1\n",
    "s = 0\n",
    "for k,v in cluster_quantity.iteritems():\n",
    "    s+=v\n",
    "for k,v in cluster_quantity.iteritems():\n",
    "    print k,int(v/float(s)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"clusters.pkl\", 'w') as datafile:\n",
    "    pickle.dump(clusters, datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kmeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b37976be9c62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create our cluster predictions for each document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'kmeans' is not defined"
     ]
    }
   ],
   "source": [
    "# Create our cluster predictions for each document\n",
    "preds = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 2, 2, 4, 4, 4, 2, 3, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could examine this further to see how it compares to original labels, but instead maybe we should just classify since we have a labelled dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying LSI Vectors:\n",
    "Let's try some simple classification on the result LSI vectors for the 20 NG set and see how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics.pairwise as smp\n",
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ng_train.target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "           metric=<function cosine_distances at 0x1077a8b18>,\n",
       "           metric_params=None, n_neighbors=3, p=2, weights='uniform')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit KNN classifier to training set with cosine distance\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79941291585127205"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score against test set\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other very cool methods!: \n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But if you really want to refine your model, you'll need more data:\n",
    "\n",
    "\n",
    "https://code.google.com/p/word2vec/\n",
    "\n",
    "Download:  'freebase-vectors-skipgram1000-en.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2vec in Models\n",
    "The output is the same type of thing that we got for LSI: a semantic space of terms.  Thus, we can do all of the same types of things that we did with those vectors (term-term, doc-doc, term-doc, ML algorithms, etc) with the added benefits of some of the geometric relationships between terms that word2vec yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Some things to keep in Mind when using Word2Vec:\n",
    "\n",
    "1) Word2vec requires a lot of data to train.\n",
    "\n",
    "As we've illustrated, you can download pretrained vectors. However, if you would need to train your own data \n",
    "you will need a lot of it!  (Think Hundreds of Millions of Words!) \n",
    "\n",
    "OTHER REFERENCES:\n",
    "\n",
    "- https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n",
    "- http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Considerations, Extensions, and Applications for LSI/Word2vec\n",
    "### Entity Extraction\n",
    "### Stopword Selection\n",
    "### Punctuation\n",
    "### Stemming\n",
    "### Alternative Weighting Schemes\n",
    "### Optimal Dimensionality Selection\n",
    "### Full Feature Utilization\n",
    "### Multilingual Corpora\n",
    "### Machine Translation\n",
    "### Language Identification\n",
    "### Majority Folding\n",
    "### Term Folding\n",
    "### Term Folding + Document Folding\n",
    "### Recommendation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
