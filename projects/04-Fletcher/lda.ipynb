{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with gensim\n",
    "We'll try out [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) in [gensim](http://radimrehurek.com/gensim/index.html) on the [20 Newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) with some simple preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"aaai_topics.pkl\", 'r') as datafile:\n",
    "    categories = pickle.load(datafile)\n",
    "\n",
    "with open(\"aaai_abstracts.pkl\", 'r') as datafile:\n",
    "    abstracts = pickle.load(datafile)\n",
    "\n",
    "for i, j in enumerate(abstracts):\n",
    "    abstracts[i]=str(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing\n",
    "We'll need to generate a term-document matrix of word (token) counts for use in LDA.\n",
    "\n",
    "We'll use `sklearn`'s `CountVectorizer` to generate our term-document matrix of counts. We'll make use of a few parameters to accomplish the following preprocessing of the text documents all within the `CountVectorizer`:\n",
    "* `analyzer=word`: Tokenize by word\n",
    "* `ngram_range=(1,2)`: Keep all 1 and 2-word grams\n",
    "* `stop_words=english`: Remove all English stop words\n",
    "* `token_pattern=\\\\b[a-z][a-z]+\\\\b`: Match all tokens with 2 or more (strictly) alphabet characters\n",
    "* `min_df=2`: Words must appear in at least 2 documents\n",
    "* `max_df=0.02`: Words must appear in less than 2% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.1, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 2), stop_words='english',\n",
    "                                  token_pattern='\\\\b[a-z][a-z]+\\\\b', max_df=0.1, min_df=5)\n",
    "count_vectorizer.fit(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1380, 398)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "ng_vecs = count_vectorizer.transform(abstracts).transpose()\n",
    "ng_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to gensim\n",
    "We need to convert our sparse `scipy` matrix to a `gensim`-friendly object called a Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(ng_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map matrix rows to words (tokens)\n",
    "We need to save a mapping (dict) of row id to word (token) for later use by gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "At this point we can simply plow ahead in creating an LDA model.  It requires our corpus of word counts, mapping of row ids to words, and the number of topics (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus, id2word=id2word, num_topics=len(categories), passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what happened.  Here are the 5 most important words for each of the 3 topics we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.013*graph + 0.012*goods + 0.012*knowledge + 0.011*agents + 0.011*level + 0.010*matching + 0.010*free + 0.010*graphs'),\n",
       " (1,\n",
       "  u'0.020*single + 0.013*human + 0.010*knowledge + 0.009*voting + 0.009*activities + 0.009*rule + 0.008*users + 0.008*sets'),\n",
       " (2,\n",
       "  u'0.030*transfer + 0.020*target + 0.015*classification + 0.015*recognition + 0.014*transfer learning + 0.014*missing + 0.013*knowledge + 0.012*target domain'),\n",
       " (3,\n",
       "  u'0.029*label + 0.024*labels + 0.022*case + 0.018*examples + 0.017*view + 0.015*classification + 0.014*labeled + 0.014*supervised'),\n",
       " (4,\n",
       "  u'0.016*bound + 0.013*behavior + 0.011*machine learning + 0.011*machine + 0.010*analysis + 0.010*class + 0.009*learning algorithms + 0.009*markov'),\n",
       " (5,\n",
       "  u'0.025*view + 0.021*objective + 0.016*multi view + 0.013*topic + 0.011*supervised + 0.009*technique + 0.009*analysis + 0.008*matrix'),\n",
       " (6,\n",
       "  u'0.021*dynamics + 0.021*probabilistic + 0.012*program + 0.012*form + 0.011*items + 0.010*online + 0.009*temporal + 0.009*schedule'),\n",
       " (7,\n",
       "  u'0.012*norm + 0.012*quality + 0.011*agents + 0.010*signals + 0.009*web + 0.008*knowledge + 0.008*feature + 0.008*local'),\n",
       " (8,\n",
       "  u'0.011*network + 0.011*size + 0.011*planning + 0.010*games + 0.010*order + 0.010*planner + 0.010*graph + 0.009*uncertainty'),\n",
       " (9,\n",
       "  u'0.018*tensor + 0.018*video + 0.013*completion + 0.011*semantics + 0.010*low + 0.008*sum + 0.008*planning + 0.007*matrices'),\n",
       " (10,\n",
       "  u'0.015*energy + 0.014*recognition + 0.012*game + 0.008*solving + 0.008*quality + 0.007*weak + 0.007*solvers + 0.007*solve'),\n",
       " (11,\n",
       "  u'0.012*mapping + 0.012*classification + 0.011*sparse + 0.010*feature + 0.009*target + 0.009*spatial + 0.008*dimensional + 0.008*simple'),\n",
       " (12,\n",
       "  u'0.034*word + 0.029*instance + 0.026*language + 0.016*neural + 0.016*classification + 0.015*qualitative + 0.014*natural language + 0.013*representations'),\n",
       " (13,\n",
       "  u'0.019*image + 0.014*images + 0.010*clustering + 0.009*selection + 0.008*dimensional + 0.008*planning + 0.007*translation + 0.007*feature'),\n",
       " (14,\n",
       "  u'0.016*decomposition + 0.013*auxiliary + 0.011*local + 0.009*influence + 0.009*scoring + 0.009*markov + 0.009*transition + 0.008*policy'),\n",
       " (15,\n",
       "  u'0.021*preferences + 0.017*nash + 0.016*preference + 0.016*mechanisms + 0.015*agent + 0.015*users + 0.013*efficiency + 0.013*properties'),\n",
       " (16,\n",
       "  u'0.015*social + 0.014*agents + 0.013*query + 0.012*logic + 0.010*planning + 0.010*content + 0.009*generation + 0.009*design'),\n",
       " (17,\n",
       "  u'0.011*kernel + 0.010*classes + 0.010*constraints + 0.009*games + 0.008*translation + 0.008*loss + 0.007*query + 0.007*evaluation'),\n",
       " (18,\n",
       "  u'0.020*social + 0.019*users + 0.014*user + 0.011*visual + 0.011*group + 0.010*prediction + 0.009*recommendation + 0.008*network'),\n",
       " (19,\n",
       "  u'0.011*inference + 0.011*relational + 0.011*heuristics + 0.011*agent + 0.010*diverse + 0.010*develop + 0.010*agents + 0.009*variables'),\n",
       " (20,\n",
       "  u'0.017*behavior + 0.014*agent + 0.013*quality + 0.011*step + 0.010*strategy + 0.010*dynamic + 0.010*behaviors + 0.008*functions'),\n",
       " (21,\n",
       "  u'0.023*selection + 0.015*feature + 0.015*behavior + 0.014*feature selection + 0.013*semi + 0.012*multiple + 0.012*analysis + 0.012*semi supervised')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=8, num_topics=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Space\n",
    "If we want to map our documents to the topic space we need to actually use the LdaModel transformer that we created above, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the document vectors in the topic space, which are measures of the component of each document along each topic.  Thus, at most a document vector can have num_topics=3 nonzero components in the topic space, and most have far fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(16, 0.9867424242288535)],\n",
       " [(6, 0.20650803173677695),\n",
       "  (10, 0.70208851743068701),\n",
       "  (16, 0.07790913262600753)],\n",
       " [(8, 0.97727272723301539)],\n",
       " [(3, 0.97552447545325294)],\n",
       " [(5, 0.98295454542590188)]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transfer learning considers related but distinct tasks defined on heterogenous domains and tries to transfer knowledge between these tasks to improve generalization performance. It is particularly useful when we do not have sufficient amount of labeled training data in some tasks, which may be very costly, laborious, or even infeasible to obtain. Instead, learning the tasks jointly enables us to effectively increase the amount of labeled training data. In this paper, we formulate a kernelized Bayesian transfer learning framework that is a principled combination of kernel-based dimensionality reduction models with task-specific projection matrices to find a shared subspace and a coupled classification model for all of the tasks in this subspace. Our two main contributions are: (i) two novel probabilistic models for binary and multiclass classification, and (ii) very efficient variational approximation procedures for these models. We illustrate the generalization performance of our algorithms on two different applications. In computer vision experiments, our method outperforms the state-of-the-art algorithms on nine out of 12 benchmark supervised domain adaptation experiments defined on two object recognition data sets. In cancer biology experiments, we use our algorithm to predict mutation status of important cancer genes from gene expression profiles using two distinct cancer populations, namely, patient-derived primary tumor data and in-vitro-derived cancer cell line data. We show that we can increase our generalization performance on primary tumors using cell lines as an auxiliary data source. '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own...\n",
    "- Go get some of the NIPS papers from [here](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words).  \n",
    "- Try performing LDA on this data with gensim\n",
    "- Play with some of the preprocessing options and parameters for LDA, observe what happens\n",
    "- See if you can use the resulting topic space to extract topic vectors and cluster some documents\n",
    "- How do your results look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
